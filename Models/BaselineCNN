python runCNN.py -n BaselineCNN -v /home/liuchen/GoogleNews-vectors-negative300.bin -d ../SST1/data -nonstatic -word2vec
##########runCNN.py############
import sys,warnings
import numpy as np

from cnnModel import *
from loadWordVec import *

warnings.filterwarnings('ignore')
sys.setrecursionlimit(40000)

def parseSentence(text,wordIndex,maxLen):
	'''
	>>>convert sentence to a matrix

	>>>type text:string
	>>>para text:raw text
	>>>type wordIndex:dict
	>>>para wordIndex:map word to its entry
	>>>type maxLen:int
	>>>para maxLen:maximum length of sentences in the whole set
	'''
	length=len(text)
	padLeft=(maxLen-length+1)/2
	padRight=(maxLen-length)/2
	vec=[]

	for i in xrange(padLeft):
		vec.append(0)
	for word in text:
		vec.append(wordIndex[word])
	for i in xrange(padRight):
		vec.append(0)

	assert len(vec)==maxLen
	return vec

def loadDatas(dataFile,wordVecFile='',dimension=300,rand=False):
	'''
	>>>load training/validate/test data and wordVec info

	>>>type dataFile/wordVecFile: string
	>>>para dataFile/wordVecFile: data and wordVec file
	>>>type dimension: int
	>>>para dimension: the dimension of word embeddings
	>>>type static: bool
	>>>para static: static wordVec or not
	'''
	fopen=open(dataFile,'rb')
	data=cPickle.load(fopen)
	fopen.close()
	sentences,vocab,config=data
		
	wordVec=[]
	if rand==False:
		vectors,wordIndex,indexWord=getWordVec(dataFile,wordVecFile)
	else:
		vectors,wordIndex,indexWord=getRandWordVec(dataFile,dimension)

	return sentences,vocab,config,vectors,wordIndex,indexWord

def parseConfig(sentences,vocab,config,vectors,wordIndex,indexWord,static,useVal,name,load):
	'''
	>>>load configs to generate model and train/validate/test batches

	>>>sentences/vocab/config/vectors/wordIndex is the same in README.md file of each dataset
	>>>type static:bool
	>>>para static:whether or not to use static wordVec
        >>>type useVal:bool
        >>>para useVal:whether or not to use validation set
	>>>type name:str
	>>>para name:model's name
        >>>type load:str
        >>>para load:model to be reloaded
	'''
	categories=config['classes']
	sets=config['all']
	train=config['train']
	validation=config['dev']
	test=config['test']
	cross=config['cross']
	dimension=len(vectors[0])
	batchSize=25

	maxLen=0
	for sentence in sentences:
		length=len(sentence['text'])
		if length>maxLen:
			maxLen=length

	setMatrix={}
	setClasses={}
	for subset in sets:
		setMatrix[subset]=[]
		setClasses[subset]=[]

	for sentence in sentences:
		vec=parseSentence(sentence['text'],wordIndex,maxLen)
		setLabel=sentence['setLabel']
		category=sentence['label']
		setMatrix[setLabel].append(vec)
		setClasses[setLabel].append(category)

	if cross==False:
		trainSet={};trainSetX=[];trainSetY=[]
		validateSet={};validationSetX=[];validationSetY=[]
		testSet={};testSetX=[];testSetY=[]
		for subset in train:
			trainSetX+=setMatrix[subset]
			trainSetY+=setClasses[subset]
		for subset in validation:
			validationSetX+=setMatrix[subset]
			validationSetY+=setClasses[subset]
		for subset in test:
			testSetX+=setMatrix[subset]
			testSetY+=setClasses[subset]

		if useVal and len(validation)==0:				#No ValidationSet
			newTrainSetX=[];newValidationSetX=[]
			newTrainSetY=[];newValidationSetY=[]

			validateEachType=int(len(trainSetX)*0.1/categories)
			validationType=[]
			for i in xrange(categories):
				validationType.append(0)

			for i in np.random.permutation(range(len(trainSetX))):
				Type=trainSetY[i]
				if validationType[Type]<validateEachType:
					newValidationSetX.append(trainSetX[i])
					newValidationSetY.append(trainSetY[i])
					validationType[Type]+=1
				else:
					newTrainSetX.append(trainSetX[i])
					newTrainSetY.append(trainSetY[i])
			# index=0
			# for i in np.random.permutation(range(len(trainSetX))):
			# 	if index<len(trainSetX)*0.9:
			# 		newTrainSetX.append(trainSetX[i])
			# 		newTrainSetY.append(trainSetY[i])
			# 	else:
			# 		newValidationSetX.append(trainSetX[i])
			# 		newValidationSetY.append(trainSetY[i])
			# 	index+=1
			trainSetX=newTrainSetX;validationSetX=newValidationSetX
			trainSetY=newTrainSetY;validationSetY=newValidationSetY
			
                if not useVal:
                        for i in xrange(len(validationSetX)):
                                trainSetX.append(validationSetX[i])
                                trainSetY.append(validationSetY[i])

		if len(trainSetX)%batchSize>0:
			extraNum=batchSize-len(trainSetX)%batchSize
			extraIndex=np.random.permutation(range(len(trainSetX)))
			for i in xrange(extraNum):
				trainSetX.append(trainSetX[extraIndex[i]])
				trainSetY.append(trainSetY[extraIndex[i]])

		if useVal and len(validationSetX)%batchSize>0:
			extraNum=batchSize-len(validationSetX)%batchSize
			extraIndex=np.random.permutation(range(len(validationSetX)))
			for i in xrange(extraNum):
				validationSetX.append(validationSetX[extraIndex[i]])
				validationSetY.append(validationSetY[extraIndex[i]])

		#trainSize=len(trainSetX)
		#validateSize=trainSize/5-(trainSize/5)%batchSize
		#validateIndex=np.random.permutation(range(trainSize))
		#for i in xrange(validateSize):
		#	validateSetX.append(trainSetX[validateIndex[i]])
		#	validateSetY.append(trainSetY[validateIndex[i]])

		trainSet['x']=np.array(trainSetX,dtype=theano.config.floatX)
		trainSet['y']=np.array(trainSetY,dtype=theano.config.floatX)
		validateSet['x']=np.array(validationSetX,dtype=theano.config.floatX)
		validateSet['y']=np.array(validationSetY,dtype=theano.config.floatX)
		testSet['x']=np.array(testSetX,dtype=theano.config.floatX)
		testSet['y']=np.array(testSetY,dtype=theano.config.floatX)

                if load=='' or load==None:
		        network=CNNModel(
                	    wordMatrix=vectors,
			    shape=(batchSize,1,maxLen,dimension),
        		    filters=(3,4,5),
			    rfilter=(5,1),
			    features=(100,),
			    time=5,categories=categories,
			    static=static,
			    dropoutRate=(0,),
			    learningRate=0.01,
			    name=name
		    )
                else:
                        network=cPickle.load(open(load,'rb'))

                try:
		        testPredictInfo,precision=network.train_validate_test(trainSet,validateSet,testSet,10)
                except:
                        timeStruct=time.localtime(time.time())
                        fileName=str(timeStruct.tm_mon)+'_'+str(timeStruct.tm_mday)+'_'+str(timeStruct.tm_hour)+'_'+str(timeStruct.tm_min)+'__'+network.name
                        cPickle.dump(network,open('../Nets/'+fileName,'wb'))
                finally:
		        network.save()

                timeStruct=time.localtime(time.time())
                fileName=str(timeStruct.tm_mon)+'_'+str(timeStruct.tm_mday)+'_'+str(timeStruct.tm_hour)+'_'+str(timeStruct.tm_min)+'__'+str(precision)+'_'+network.name
                cPickle.dump(network,open('../Nets/'+fileName,'wb'))
                testPredictInfo['indexWord']=indexWord
                testPredictInfo['testSet']=testSet
                cPickle.dump(testPredictInfo,open('../Errors/'+fileName,'wb'))
		
                print 'Model '+name+' :Final Precision Rate %f%%'%(precision*100.)
	else:
		precisions=[]
                testPredicts=[]
		for item in sets:
			trainSet={};trainSetX=[];trainSetY=[]
			validateSet={};validationSetX=[];validationSetY=[]
			testSet={};testSetX=[];testSetY=[]
			for subset in sets:
				if item!=subset:
					trainSetX+=setMatrix[subset]
					trainSetY+=setClasses[subset]
				else:
					testSetX+=setMatrix[subset]
					testSetY+=setClasses[subset]

                        if useVal:#No ValidationSet
			        newTrainSetX=[];newValidationSetX=[]
        			newTrainSetY=[];newValidationSetY=[]

	        		validateEachType=int(len(trainSetX)*0.1/categories)
		        	validationType=[]
        			for i in xrange(categories):
	        			validationType.append(0)

		        	for i in np.random.permutation(range(len(trainSetX))):
			        	Type=trainSetY[i]
				        if validationType[Type]<validateEachType:
        					newValidationSetX.append(trainSetX[i])
	        				newValidationSetY.append(trainSetY[i])
		        			validationType[Type]+=1
			        	else:
				        	newTrainSetX.append(trainSetX[i])
					        newTrainSetY.append(trainSetY[i])

        			trainSetX=newTrainSetX;validationSetX=newValidationSetX
	        		trainSetY=newTrainSetY;validationSetY=newValidationSetY


			if len(trainSetX)%batchSize>0:
				extraNum=batchSize-len(trainSetX)%batchSize
				extraIndex=np.random.permutation(range(len(trainSetX)))
				for i in xrange(extraNum):
					trainSetX.append(trainSetX[extraIndex[i]])
					trainSetY.append(trainSetY[extraIndex[i]])

			if useVal and len(validationSetX)%batchSize>0:
				extraNum=batchSize-len(validationSetX)%batchSize
				extraIndex=np.random.permutation(range(len(validationSetX)))
				for i in xrange(extraNum):
					validationSetX.append(validationSetX[extraIndex[i]])
					validationSetY.append(validationSetY[extraIndex[i]])

			trainSet['x']=np.array(trainSetX,dtype=theano.config.floatX)
			trainSet['y']=np.array(trainSetY,dtype=theano.config.floatX)
			validateSet['x']=np.array(validationSetX,dtype=theano.config.floatX)
			validateSet['y']=np.array(validationSetY,dtype=theano.config.floatX)
			testSet['x']=np.array(testSetX,dtype=theano.config.floatX)
			testSet['y']=np.array(testSetY,dtype=theano.config.floatX)

                        if load=='' or load==None:
        		        network=CNNModel(
	        		    wordMatrix=vectors,
		        	    shape=(batchSize,1,maxLen,dimension),
        			    filters=(3,4,5),
				    rfilter=(5,1),
				    features=(100,),
				    time=1,categories=categories,
				    static=static,
				    dropoutRate=(0.5,),
				    learningRate=0.01,
				    name=name
        			)
                        else:
                                network=cPickle.load(open(load,'rb'))

                        try:
			        testPredictInfo,precision=network.train_validate_test(trainSet,validateSet,testSet,10)
                        except:
                                timeStruct=time.localtime(time.time())
                                fileName=str(timeStruct.tm_mon)+'_'+str(timeStruct.tm_mday)+'_'+str(timeStruct.tm_hour)+'_'+str(timeStruct.tm_min)+'__'+network.name
                                cPickle.dump(network,open(fileName,'wb'))
                        finally:
                                network.save()

                        timeStruct=time.localtime(time.time())
                        fileName=str(timeStruct.tm_mon)+'_'+str(timeStruct.tm_mday)+'_'+str(timeStruct.tm_hour)+'_'+str(timeStruct.tm_min)+'__'+str(precision)+'_'+network.name
                        cPickle.dump(network,open('../Nets/'+fileName,'wb'))
			precisions.append(precision)
                        testPredictInfo['indexWord']=indexWord
                        testPredictInfo['testSet']=testSet
                        testPredicts.append(testPredictInfo)

                timeStruct=time.localtime(time.time())
                fileName=str(timeStruct.tm_mon)+'_'+str(timeStruct.tm_mday)+'_'+str(timeStruct.tm_hour)+'_'+str(timeStruct.tm_min)+'__'+str(precision)+'_'+network.name 
                cPickle.dump(testPredicts,open('../Errors/'+fileName,'wb'))
                print 'Model '+name+' :Final Precision Rate %f%%'%(np.mean(precisions)*100.)

if __name__=='__main__':
	static=False
	rand=False
	mode=0
	dataFile=''
	vecFile=''
	name=''
        useVal=True
        load=''

	for i in xrange(len(sys.argv)):
		if i==0:
			continue
		if sys.argv[i]=='-d':
			mode=1
		elif sys.argv[i]=='-v':
			mode=2
		elif sys.argv[i]=='-n':
			mode=3
                elif sys.argv[i]=='-r':
                        mode=4
		else:
			if mode==1:
				dataFile=sys.argv[i]
				mode=0
			elif mode==2:
				vecFile=sys.argv[i]
				mode=0
			elif mode==3:
				name=sys.argv[i]
				mode=0
                        elif mode==4:
                                load=sys.argv[i]
                                mode=0
			else:
				if sys.argv[i]=='-nonstatic':
					static=False
				elif sys.argv[i]=='-static':
					static=True
				elif sys.argv[i]=='-rand':
					rand=True
				elif sys.argv[i]=='-word2vec':
					rand=False
                                elif sys.argv[i]=='noval':
                                        useVal=False
				else:
					raise NotImplementedError('command line error')
	print 'config: dataFile:%s, vecFile:%s, static:%r, rand:%r'%(dataFile,vecFile,static,rand)

	saveFile='../Models/'+name
	fwrite=open(saveFile,'w')
	cmd='python'
        for item in sys.argv:
                cmd+=' '+item
        fwrite.write(cmd+'\n')
	fwrite.write('##########runCNN.py############\n')
	with open('runCNN.py','r') as fopen:
		for line in fopen:
			fwrite.write(line)
	fwrite.write('##########cnnModel.py#############\n')
	with open('cnnModel.py','r') as fopen:
		for line in fopen:
			fwrite.write(line)
	fwrite.close()
	print 'model '+name+' saved!'
	sentences,vocab,config,vectors,wordIndex,indexWord=loadDatas(dataFile=dataFile,wordVecFile=vecFile,dimension=300,rand=rand)
	parseConfig(sentences,vocab,config,vectors,wordIndex,indexWord,static,useVal,name,load)
##########cnnModel.py#############
import cPickle,sys
import time
import numpy as np
import theano
import theano.tensor as T
from collections import defaultdict, OrderedDict

from loadWordVec import *
from hiddenLayer import *
from convLayer import *
from logisticRegression import *

sys.setrecursionlimit(40000)

def ReLU(x):
	return T.switch(x<0,0,x)

def as_floatX(variable):
	if isinstance(variable,float) or isinstance(variable,np.ndarray):
		return np.cast[theano.config.floatX](variable)
	return T.cast(variable,theano.config.floatX)

def AdadeltaUpdate(params,cost,rho=0.95,epsilon=1e-6,norm_lim=9):
	'''
	>>>

	>>>type params: tuple or list
	>>>para params: parameters
	>>>type cost:
	>>>para cost:
	>>>type rho: float
	>>>para rho:
	>>>type epsilon: float
	>>>para epsilon:
	>>>type norm_lim: int
	>>>para norm_lim:
	'''
	updates=OrderedDict({})
	exp_sqr_grads=OrderedDict({})
	exp_sqr_update=OrderedDict({})
	g_params=[]
	for param in params:
		empty=np.zeros_like(param.get_value())
		exp_sqr_grads[param]=theano.shared(value=as_floatX(empty),name='exp_grad_%s'%param.name)
		exp_sqr_update[param]=theano.shared(value=as_floatX(empty),name='exp_grad_%s'%param.name)
		gp=T.grad(cost,param)
		g_params.append(gp)
	for param,gp in zip(params,g_params):
		exp_sg=exp_sqr_grads[param]
		exp_su=exp_sqr_update[param]
		update_exp_sg=rho*exp_sg+(1-rho)*T.sqr(gp)#????
		updates[exp_sg]=update_exp_sg
		
		step=-(T.sqrt(exp_su+epsilon)/T.sqrt(update_exp_sg+epsilon))*gp		
		stepped_param=param+step

		update_exp_su=rho*exp_su+(1-rho)*T.sqr(step)
		updates[exp_su]=update_exp_su

		if param.get_value(borrow=True).ndim==2 and param.name!='wordVec':
			col_norms=T.sqrt(T.sum(T.sqr(stepped_param),axis=0))
			desired_norms=T.clip(col_norms,0,T.sqrt(norm_lim))#???
			scale=desired_norms/(1e-7+col_norms)
			updates[param]=stepped_param*scale
		else:
			updates[param]=stepped_param
	return updates

class CNNModel(object):
	
	def __init__(self,wordMatrix,shape,filters,rfilter,features,time,
			categories,static,dropoutRate,learningRate,name):
		'''
		>>>initalize the model

		>>>type wordMatrix: matrix
		>>>para wordMatrix: input tensor
		>>>type shape: tuple or list of length 4
		>>>para shape: [batchSize,featureMaps,sentenceLen,dimension]
		>>>type filters: tuple or list of int
		>>>para filters: different sizes of filters
		>>>type rfilter: tuple of list of length 2
		>>>para rfilter: the filter size of recurrent connection
		>>>type features: tuple or list of int
		>>>para features: num of feature maps in each layer
		>>>type time: int
		>>>para time: the iteration times of recurrent connection
		>>>type categories: int
		>>>para categories: target categories
		>>>type static: boolean
		>>>para static: static wordVec or not
		>>>type dropoutRate: tuple of list of float
		>>>para dropoutRate: dropout rate of each layer
		>>>type learningRate: float
		>>>para learningRate: learning rate
		>>>type name: str
		>>>para name: model's name
		'''
		self.learningRate=learningRate
		self.static=static
		self.name=name
                self.categories=categories

		rng=np.random.RandomState(2011010539)
		self.batchSize,featureMaps,self.sentenceLen,self.dimension=shape

		filterSizes=[]
		poolSizes=[]
		for filter in filters:
			filterSizes.append([features[0],featureMaps,filter,self.dimension])
			poolSizes.append([self.sentenceLen-filter+1,1])

		#build up the model
		self.x=T.matrix('x')		#batch sentences
		self.y=T.ivector('y')		#output labels
		self.lr=T.dscalar('lr')

		self.wordVec=theano.shared(wordMatrix,name='wordVec')

		input=self.wordVec[T.cast(self.x.flatten(),dtype='int32')].reshape(shape)

		self.layers0=[]
		layer1Inputs=[]
		for i in xrange(len(filters)):
			filterSize=filterSizes[i]
			poolSize=poolSizes[i]
			ConvLayer=ConvPool(
				rng=rng,
				input=input,
				shape=shape,
				filters=filterSize,
				pool=poolSize
			)
			self.layers0.append(ConvLayer)
			layer1Inputs.append(ConvLayer.output.flatten(2))

		self.layer1=LogisticRegression(
			input=T.concatenate(layer1Inputs,1),
			n_in=len(filters)*features[0],
			n_out=categories,
		)

		self.params=self.layer1.param
		for layer in self.layers0:
			self.params+=layer.param
		if static==False:
			self.params+=[self.wordVec]

		weights=0
		for param in self.layer1.param:
			weights+=T.sum(T.sqr(param))

		self.cost=self.layer1.negative_log_likelyhood(self.y)
		self.errors=self.layer1.errors(self.y)

		#for key in self.params:
		#	print key.name,key.get_value().shape
		grads=T.grad(self.cost,self.params)
		self.update=[
			(paramI,paramI-gradI*self.lr)
			for (paramI,gradI) in zip(self.params,grads)
		]
		self.adadeltaUpdate=AdadeltaUpdate(self.params,self.cost)

		print 'the model '+self.name+' constructed!'

	def train_validate_test(self,trainSet,validateSet,testSet,nEpoch):
		'''
		>>>train and test the model

		>>>type trainSet/validateSet/testSet: matrix
		>>>para trainSet/validateSet/testSet: different subset

		>>>type nEpoch: int
		>>>para nEpoch: maximum iteration epoches
		'''
		print trainSet['x'].shape
		trainSize=trainSet['x'].shape[0]
		validateSize=validateSet['x'].shape[0]
		testSize=testSet['x'].shape[0]
		trainX=theano.shared(trainSet['x'],borrow=True)
		trainY=theano.shared(trainSet['y'],borrow=True)
		trainY=T.cast(trainY,'int32')
		validateX=theano.shared(validateSet['x'],borrow=True)
		validateY=theano.shared(validateSet['y'],borrow=True)
		validateY=T.cast(validateY,'int32')
		testX=testSet['x']
		testY=np.asarray(testSet['y'],'int32')
		trainBatches=trainSize/self.batchSize
		validateBatches=validateSize/self.batchSize

		index=T.iscalar('index')
		testMatrix=T.matrix('WordMatrix')
		testLabel=T.iscalar('TestLabel')
		learnRate=T.scalar('lr')

		trainModel=theano.function(
		[index],self.cost,updates=self.adadeltaUpdate,
		givens={
		self.x:trainX[index*self.batchSize:(index+1)*self.batchSize],
		self.y:trainY[index*self.batchSize:(index+1)*self.batchSize]})
		print 'training model constructed!'

		testTrain=theano.function(
		[index],[self.cost,self.errors],
		givens={
		self.x:trainX[index*self.batchSize:(index+1)*self.batchSize],
		self.y:trainY[index*self.batchSize:(index+1)*self.batchSize]})
		print 'test training set model constructed!'

		validateModel=theano.function(
		[index],self.errors,
		givens={
		self.x:validateX[index*self.batchSize:(index+1)*self.batchSize],
		self.y:validateY[index*self.batchSize:(index+1)*self.batchSize]})
		print 'validation model constructed!'

		testLayer0Output=[]
		testLayer0Input=self.wordVec[T.cast(self.x.flatten(),dtype='int32')].reshape((testSize,1,self.sentenceLen,self.dimension))
		for layer in self.layers0:
			output=layer.process(testLayer0Input,testSize)
			testLayer0Output.append(output.flatten(2))
		testLayer1Input=T.concatenate(testLayer0Output,1)
		testPredict=self.layer1.predictInstance(testLayer1Input)
		testError=T.mean(T.neq(testPredict,self.y))
		testModel=theano.function([self.x,self.y],[testPredict,testError])
		print 'testing model constructed!'

		epoch=0
		iteration=0
		maxIteration=10000
		minError=1.0
		rate=0.01
		bestValPrecision=0.0
		finalPrecision=0.0
		self.trainAcc=[]
		self.validateAcc=[]
		self.testAcc=[]
		self.costValue=[]
		self.result={'minError':1.00,'finalAcc':0.00,'bestValAcc':0.00}
                testPredict=np.zeros(shape=(testSize,))
                predictMatrix=np.zeros(shape=(self.categories,self.categories),dtype='int32')   #predictMatrix[test][predict]

		while epoch<nEpoch and iteration<maxIteration:
			epoch+=1
			num=0
			for minBatch in np.random.permutation(range(trainBatches)):
				cost=trainModel(minBatch)				#set zero func
				x=float(epoch)+float(num+1)/float(trainBatches)-1
				#self.costValue.append({'x':x,'value':cost})
				if num%50==0:
					trainResult=[testTrain(i) for i in xrange(trainBatches)]
                                        trainCost,trainError=np.mean(trainResult,axis=0)
					trainPrecision=1-trainError
                                        self.costValue.append({'x':x,'value':trainCost})
					validateError=[validateModel(i) for i in xrange(validateBatches)]
					validatePrecision=1-np.mean(validateError)
					print 'epoch=%i,num=%i,train precision=%f%%, validation precision=%f%%'%(epoch,num,trainPrecision*100.,validatePrecision*100.)
					self.trainAcc.append({'x':x,'acc':trainPrecision})
					self.validateAcc.append({'x':x,'acc':validatePrecision})
					if validatePrecision>bestValPrecision:
						testPredict,testError=testModel(testX,testY)
                                                assert len(testPredict)==len(testY)
                                                predictMatrix=np.zeros(shape=(self.categories,self.categories),dtype='int32')
                                                for case in xrange(len(testY)):
                                                    predictMatrix[testY[case],testPredict[case]]+=1
						testPrecision=1-testError
						minError=min(minError,testError)					
						finalPrecision=testPrecision
						bestValPrecision=validatePrecision
						print 'testing precision=%f%%'%(testPrecision*100.)
						self.testAcc.append({'x':x,'acc':testPrecision})
					print 'bestValPrecision=%f%%'%(bestValPrecision*100.)
				num+=1

			x=float(epoch)
			trainResult=[testTrain(i) for i in xrange(trainBatches)]
                        trainCost,trainError=np.mean(trainResult,axis=0)
                        trainPrecision=1-trainError
                        self.costValue.append({'x':x,'value':trainCost})
			validateError=[validateModel(i) for i in xrange(validateBatches)]
			validatePrecision=1-np.mean(validateError)
			print 'epoch=%i,train precision=%f%%, validation precision=%f%%'%(epoch,trainPrecision*100.,validatePrecision*100.)
			self.trainAcc.append({'x':x,'acc':trainPrecision})
			self.validateAcc.append({'x':x,'acc':validatePrecision})
			if validatePrecision>bestValPrecision:
				testPredict,testError=testModel(testX,testY)
                                assert len(testPredict)==len(testY)
                                predictMatrix=np.zeros(shape=(self.categories,self.categories),dtype='int32')
                                for case in xrange(len(testY)):
                                    predictMatrix[testY[case],testPredict[case]]+=1
				testPrecision=1-testError
				minError=min(minError,testError)
				finalPrecision=testPrecision
				bestValPrecision=validatePrecision
				print 'testing precision=%f%%'%(testPrecision*100.)
				self.testAcc.append({'x':x,'acc':testPrecision})
			print 'bestValPrecision=%f%%'%(bestValPrecision*100.)
			print 'bestTestPrecision=%f%%, finalPrecision=%f%%'%((1-minError)*100.,finalPrecision*100.)

		self.result={'minError':minError,'finalAcc':finalPrecision,'bestValAcc':bestValPrecision}
                testPredictInfo={'testPredict':testPredict,'predictMatrix':predictMatrix}

		return testPredictInfo,finalPrecision

	def save(self):
		savePath='../Results/'
		timeStruct=time.localtime(time.time())
		fileName=str(timeStruct.tm_mon)+'_'+str(timeStruct.tm_mday)+'_'+str(timeStruct.tm_hour)+'_'+str(timeStruct.tm_min)+'__'+str(self.result['finalAcc'])+'_'+self.name
		cPickle.dump([self.result,self.trainAcc,self.validateAcc,self.testAcc,self.costValue],open(savePath+fileName,'wb'))
